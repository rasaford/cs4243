import numpy as np
from skimage import filters
from skimage.feature import corner_peaks
from scipy.spatial.distance import cdist
from scipy.ndimage.filters import convolve
import math

from utils import pad, get_output_space, warp_image


def harris_corners(img, window_size=3, k=0.04):
    """
    Compute Harris corner response map. Follow the math equation
    R=Det(M)-k(Trace(M)^2).

    Hint:
        You may use the function scipy.ndimage.filters.convolve, 
        which is already imported above

    Args:
        img: Grayscale image of shape (H, W)
        window_size: size of the window function
        k: sensitivity parameter

    Returns:
        response: Harris response image of shape (H, W)
    """

    H, W = img.shape
    window = np.ones((window_size, window_size))

    response = np.zeros((H, W))

    dx = filters.sobel_v(img)
    dy = filters.sobel_h(img)

    # create weight matrix (only works for odd kernel sizes)
    w = np.zeros((window_size, window_size))
    w[window_size//2, window_size//2] = 1
    w = filters.gaussian(w, sigma=3)

    # filter derivates with kernel
    dx2 = convolve(dx * dx, w)
    dy2 = convolve(dy * dy, w)
    dxy = convolve(dx * dy, w)

    for y in range(H):
        for x in range(W):
            det = dx2[y, x] * dy2[y, x] - dxy[y, x] * dxy[y, x]
            trace = k * (dx2[y, x] + dy2[y, x]) ** 2
            response[y, x] = det - trace

    return response


def simple_descriptor(patch):
    """
    Describe the patch by normalizing the image values into a standard 
    normal distribution (having mean of 0 and standard deviation of 1) 
    and then flattening into a 1D array. 

    The normalization will make the descriptor more robust to change 
    in lighting condition.

    Hint:
        If a denominator is zero, divide by 1 instead.

    Args:
        patch: grayscale image patch of shape (h, w)

    Returns:
        feature: 1D array of shape (h * w)
    """

    h, w = patch.shape
    patch = np.reshape(patch, (h * w))
    mean = np.mean(patch, dtype=np.float64)
    std = np.std(patch, dtype=np.float64)
    return (patch - mean) / std


def describe_keypoints(image, keypoints, desc_func, patch_size=16):
    """
    Args:
        image: grayscale image of shape (H, W)
        keypoints: 2D array containing a keypoint (y, x) in each row
        desc_func: function that takes in an image patch and outputs
            a 1D feature vector describing the patch
        patch_size: size of a square patch at each keypoint

    Returns:
        desc: array of features describing the keypoints
    """

    image.astype(np.float32)
    desc = []

    for i, kp in enumerate(keypoints):
        y, x = kp
        patch = image[y-(patch_size//2):y+((patch_size+1)//2),
                      x-(patch_size//2):x+((patch_size+1)//2)]
        desc.append(desc_func(patch))
    return np.array(desc)


def match_descriptors(desc1, desc2, threshold=0.5):
    """
    Match the feature descriptors by finding distances between them. A match is formed 
    when the distance to the closest vector is much smaller than the distance to the 
    second-closest, that is, the ratio of the distances should be smaller
    than the threshold. Return the matches as pairs of vector indices.

    Hint: 
        You may use `scipy.spatial.distance.cdist` to compute distance between desc1 
        and desc2, which is already imported above

    Args:
        desc1: an array of shape (M, P) holding descriptors of size P about M keypoints
        desc2: an array of shape (N, P) holding descriptors of size P about N keypoints

    Returns:
        matches: an array of shape (Q, 2) where each row holds the indices of one pair 
        of matching descriptors
    """
    matches = []

    M = desc1.shape[0]
    N = desc2.shape[0]

    distances = cdist(desc1, desc2)

    matches = []
    for m in range(M):
        min_idx = (m, np.argpartition(distances[m], 0)[0])
        min2_idx = (m, np.argpartition(distances[m], 1)[1])
        if distances[min_idx] / distances[min2_idx] < threshold:
            matches.append(min_idx)

    return np.array(matches)


def fit_affine_matrix(p1, p2):
    """ Fit affine matrix such that p2 * H = p1 

    Args:
        p1: an array of shape (M, P)
        p2: an array of shape (M, P)

    Return:
        H: a matrix of shape (P * P) that transform p2 to p1.
    """

    assert (p1.shape[0] == p2.shape[0]),\
        'Different number of points in p1 and p2'
    p1 = pad(p1)
    p2 = pad(p2)

    H = np.linalg.lstsq(p2, p1)[0]

    # Sometimes numerical issues cause least-squares to produce the last
    # column which is not exactly [0, 0, 1]
    H[:, 2] = np.array([0, 0, 1])
    return H


def ransac(keypoints1, keypoints2, matches, n_iters=200, threshold=20):
    """
    Use RANSAC to find a robust affine transformation

        1. Select random set of matches
        2. Compute affine transformation matrix
        3. Compute inliers
        4. Keep the largest set of inliers
        5. Re-compute least-squares estimate on all of the inliers

    Args:
        keypoints1: M1 x 2 matrix, each row is a point
        keypoints2: M2 x 2 matrix, each row is a point
        matches: N x 2 matrix, each row represents a match
            [index of keypoint1, index of keypoint 2]
        n_iters: the number of iterations RANSAC will run
        threshold: the number of threshold to find inliers

    Returns:
        H: a robust estimation of affine transformation from keypoints2 to
        keypoints 1
    """
    N = matches.shape[0]
    n_samples = int(N * 0.2)

    matched1 = pad(keypoints1[matches[:, 0]])
    matched2 = pad(keypoints2[matches[:, 1]])

    max_inliers = np.zeros(N)

    n_inliers = 0
    H = None

    for i in range(n_iters):
        # draw random matches
        rand_idx = np.random.randint(N, size=4)
        pts_idx = matches[rand_idx]
        p1 = keypoints1[pts_idx[:, 0]]
        p2 = keypoints2[pts_idx[:, 1]]

        # fit model  p2 * H = p1
        Hp = fit_affine_matrix(p1, p2)

        # compute inliers
        inliers = np.array([idx for idx, (m1, m2)
                            in enumerate(zip(np.dot(matched2, Hp), matched1))
                            if np.linalg.norm(m1 - m2) < threshold])

        # maximize number of inliers
        if inliers.shape[0] > n_inliers:
            H = Hp
            n_inliers = inliers.shape[0]
            max_inliers = rand_idx

    return H, matches[max_inliers]


def sift_descriptor(patch):
    """
    Implement a simplifed version of Scale-Invariant Feature Transform (SIFT).
    Paper reference: https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf

    Your implementation does not need to exactly match the SIFT reference.
    Here are the key properties your (baseline) descriptor should have:
    (1) a 4x4 grid of cells, each length of 16/4=4. It is simply the
        terminology used in the feature literature to describe the spatial
        bins where gradient distributions will be described.
    (2) each cell should have a histogram of the local distribution of
        gradients in 8 orientations. Appending these histograms together will
        give you 4x4 x 8 = 128 dimensions.
    (3) Each feature should be normalized to unit length.

    You do not need to perform the interpolation in which each gradient
    measurement contributes to multiple orientation bins in multiple cells
    As described in Szeliski, a single gradient measurement creates a
    weighted contribution to the 4 nearest cells and the 2 nearest
    orientation bins within each cell, for 8 total contributions. This type
    of interpolation probably will help, though.

    You do not have to explicitly compute the gradient orientation at each
    pixel (although you are free to do so). You can instead filter with
    oriented filters (e.g. a filter that responds to edges with a specific
    orientation). All of your SIFT-like feature can be constructed entirely
    from filtering fairly quickly in this way.

    You do not need to do the normalize -> threshold -> normalize again
    operation as detailed in Szeliski and the SIFT paper. It can help, though.

    Another simple trick which can help is to raise each element of the final
    feature vector to some power that is less than one.

    Args:
        patch: grayscale image patch of shape (h, w)

    Returns:
        feature: 1D array of shape (128, )
    """

    dx = filters.sobel_v(patch)
    dy = filters.sobel_h(patch)
    histogram = np.zeros((4, 4, 8))

    h, w = patch.shape[:2]
    for y in range(h // 4):
        for x in range(w // 8):
            dy_w = dy[y * 4: (y + 1) * 4,
                      x * 4: (x + 1) * 4].ravel()
            dx_w = dx[y * 4: (y + 1) * 4,
                      x * 4: (x + 1) * 4].ravel()
            mag_w = np.sqrt(dx_w * dx_w + dy_w * dy_w)
            dirs = ((np.arctan2(dy_w, dx_w) + np.pi * 4)
                    / np.pi).astype(int)
            histogram[y, x] = np.bincount(dirs, weights=mag_w, minlength=8)

    feature = np.reshape(histogram, (128,))
    feature /= np.linalg.norm(feature)
    return feature

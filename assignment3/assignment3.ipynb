{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS4243 Assignment 3\n",
    "\n",
    "The goal of this assignment is to do image stitching -- how multiple images can form a panorama. To do this, you would need to implement feature detectors, feature descriptors, and keypoints matching algorithms (transformation estimation is done for your convenience).\n",
    "\n",
    "## Assignment Guideline (Carefully Read Before Coding)\n",
    "\n",
    "Please create a new environment (do **NOT** use the same environment as in previous assignments to prevent from version conflict) and install all requirements with `pip install -r requirements.txt`. You are required to finish this notebook by successfully **(1) running all of the segments, (2) producing results and (3) answering all of the questions**. \n",
    "\n",
    "In this assignment, you are required to implement the key components of each step by filling the missing code segments in `image_stitching.py`. You can edit `image_stitching.py` with any editor/IDE (e.g. PyCharm) you are comfortable with. Before running the code segments, make sure the notebook are running in a virtual environment that has installed all required packages in `requirements.txt`. After you finish filling a code segment, make sure to save it before you run the related cells of this notebook (the notebook will automatically re-load your code in `image_stitching.py`) to check if the results are correct.\n",
    "\n",
    "### Submission Guideline\n",
    "\n",
    "After you finish the code and get all the results, **(1) re-run this notebook (and make sure you get all the clean outputs), (2) zip the same folder back and name it with your student ID (e.g. A0123456H.zip) and (3) submit to the NUS Luminus system (in time, before 23h59 on 26 Sep 2019)**. Please be careful since any failure to follow the submission guideline will result in a grade deduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "from skimage import filters\n",
    "from skimage.feature import corner_peaks\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15.0, 12.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal: Image Stitching\n",
    "\n",
    "Image stitching is the process of combining multiple photographic images with overlapping fields of view to produce a segmented panorama or high-resolution image. Since Brown Matthew and David G. Lowe published [Automatic Panoramic Image Stitching using Invariant Features](http://matthewalunbrown.com/papers/ijcv2007.pdf) in 2007, this technique has quickly been widely-used in many applications such as Google Street View, panaroma function in smartphones and image stitching softwares.\n",
    "\n",
    "This assignment consists of the following parts:\n",
    "\n",
    "1. Detecting keypoints with corner detectors (Harris, etc.). (**16 points**)\n",
    "\n",
    "2. Extracting local (simple) descriptors from two input images and compare two sets of descriptors coming from two different images and find matching keypoints. (**40 points incl. discussion**) Then use least-squares method to find the affine transformation matrix that maps points in one image to another, and use it to transform the second image and overlay it on the first image, forming a panorama. \n",
    "    - Discussion #1 (6 points)\n",
    "\n",
    "3. Try RANSAC descriptor matching algorithm to get a better result (**22 points incl. discussion**)\n",
    "   - Discussion #2 (6 points)\n",
    "\n",
    "4. Implement a different descriptor (SIFT) to see the results. (**22 points incl. discussion**)\n",
    "   - Discussion #3 (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(imread('marina_bay.jpg'))\n",
    "plt.axis('off')\n",
    "plt.title('Example paranoma of Singapore\\'s Marina Bay. Credit to https://flic.kr/p/25SmXVL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Harris Corner Detector\n",
    "\n",
    "For keypoints detector, we adopt Harris Corner Detector. Refer to the lecture slides if you are still not familiar with it. Basically, there are 5 key steps in the algorithm:\n",
    "\n",
    "1. Compute derivatives in x and y directions respectively of an image: ($I_x, I_y$) \n",
    "2. Compute products of derivatives ($I_x^2, I_y^2, I_{xy}$) at each pixel\n",
    "3. Compute matrix $M$ at each pixel, where\n",
    "$$\n",
    "M = \\sum_{x,y} w(x,y)\n",
    "    \\begin{bmatrix}\n",
    "        I_{x}^2 & I_{x}I_{y} \\\\\n",
    "        I_{x}I_{y} & I_{y}^2\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "<br>\n",
    "4. Compute corner response $R=Det(M)-k(Trace(M)^2)$ at each pixel\n",
    "5. Output corner response map $R(x,y)$\n",
    "\n",
    "Step 1 is already done for you in the function **`harris_corners`** in `image_stitching.py`. \n",
    "\n",
    "**Implement `harris_corners` function and then run the code cells below.**\n",
    "\n",
    "*-Hint: You may use the function `scipy.ndimage.filters.convolve`, which is already imported in `image_stitching.py`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Before working on the corner detectors, let's see how the original images look like\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(imread('letterbox.jpg'))\n",
    "plt.axis('off')\n",
    "plt.title('letter box')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(imread('checker.jpg'))\n",
    "plt.axis('off')\n",
    "plt.title('checker')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from image_stitching import harris_corners\n",
    "\n",
    "img1 = imread('letterbox.jpg', as_grey=True)\n",
    "img2 = imread('checker.jpg', as_grey=True)\n",
    "\n",
    "# Compute Harris corner response\n",
    "response1 = harris_corners(img1)\n",
    "response2 = harris_corners(img2)\n",
    "\n",
    "# Display corner response\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(response1)\n",
    "plt.axis('off')\n",
    "plt.title('Harris Corner Response on letter box')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(imread('solution_harris_letterbox.jpg', as_grey=True))\n",
    "plt.axis('off')\n",
    "plt.title('Harris Corner Solution on letter box')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow(response2)\n",
    "plt.axis('off')\n",
    "plt.title('Harris Corner Response on checker')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.imshow(imread('solution_harris_checker.jpg', as_grey=True))\n",
    "plt.axis('off')\n",
    "plt.title('Harris Corner Solution on checker')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform non-maximum suppression in response map\n",
    "# and output corner coordiantes\n",
    "corners1 = corner_peaks(response1, threshold_rel=0.01)\n",
    "corners2 = corner_peaks(response2, threshold_rel=0.01)\n",
    "\n",
    "# Display detected corners\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1)\n",
    "plt.scatter(corners1[:,1], corners1[:,0], marker='x')\n",
    "plt.axis('off')\n",
    "plt.title('Detected Corners on letter box')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2)\n",
    "plt.scatter(corners2[:,1], corners2[:,0], marker='x')\n",
    "plt.axis('off')\n",
    "plt.title('Detected Corners on checker')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Extracting and Matching keypoints (40 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2a: Keypoint Descriptor (17 points)\n",
    "\n",
    "Now we are able to detect keypoints by running Harris corner detector independently on two images. However, how do we know which pair of keypoints come from the same location in those two images? In order to *match* the keypoints, we need to first *describe* the keypoints based on their local appearance. A simple idea is to take the region around the keypoint and convert into a fix-sized vector called **descriptor**. \n",
    "\n",
    "We first perform Harris corner detector on the two images we are going to work on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from image_stitching import harris_corners\n",
    "\n",
    "img1 = imread('marinabay_a.jpg', as_grey=True)\n",
    "img2 = imread('marinabay_b.jpg', as_grey=True)\n",
    "\n",
    "# Detect keypoints in two images\n",
    "keypoints1 = corner_peaks(harris_corners(img1, window_size=3),\n",
    "                          threshold_rel=0.05,\n",
    "                          exclude_border=8)\n",
    "keypoints2 = corner_peaks(harris_corners(img2, window_size=3),\n",
    "                          threshold_rel=0.05,\n",
    "                          exclude_border=8)\n",
    "\n",
    "# Display detected keypoints\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1)\n",
    "plt.scatter(keypoints1[:,1], keypoints1[:,0], marker='x')\n",
    "plt.axis('off')\n",
    "plt.title('Detected Keypoints for Image 1')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2)\n",
    "plt.scatter(keypoints2[:,1], keypoints2[:,0], marker='x')\n",
    "plt.axis('off')\n",
    "plt.title('Detected Keypoints for Image 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement `simple_descriptor` function in `image_stitching.py`** before proceeding to the next section. The function will be used in the following descriptors matching.\n",
    "\n",
    "*Hint: In **`simple_descriptor`** each keypoint is described by normalized intensity in a small patch around it.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2b: Descriptors Matching (17 points)\n",
    "\n",
    "Then, implement **`match_descriptors`** function to find good matches in two sets of descriptors. First, calculate Euclidean distance between all pairs of descriptors from image 1 and image 2. Then use this to determine if there is a good match: if the distance to the closest vector is significantly (by a factor which is given) smaller than the distance to the second-closest, we call it a match. The output of the function is an array where each row holds the indices of one pair of matching descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from image_stitching import simple_descriptor, match_descriptors, describe_keypoints\n",
    "from utils import plot_matches\n",
    "\n",
    "patch_size = 5\n",
    "\n",
    "# Extract features from the corners\n",
    "desc1 = describe_keypoints(img1, keypoints1,\n",
    "                           desc_func=simple_descriptor,\n",
    "                           patch_size=patch_size)\n",
    "desc2 = describe_keypoints(img2, keypoints2,\n",
    "                           desc_func=simple_descriptor,\n",
    "                           patch_size=patch_size)\n",
    "\n",
    "# Match descriptors in image1 to those in image2\n",
    "matches = match_descriptors(desc1, desc2, 0.7)\n",
    "\n",
    "# Plot matches\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
    "ax.axis('off')\n",
    "plot_matches(ax, img1, img2, keypoints1, keypoints2, matches)\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('solution_simple_descriptor.jpg'))\n",
    "plt.axis('off')\n",
    "plt.title('Matched Simple Descriptor Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation Estimation\n",
    "\n",
    "We now have a list of matched keypoints across the two images. We will use this to find a transformation matrix that maps points in the second image to the corresponding coordinates in the first image. In other words, if the point $p_1 = [y_1,x_1]$ in image 1 matches with $p_2=[y_2, x_2]$ in image 2, we need to find an affine transformation matrix $H$ such that\n",
    "$$\n",
    "\\tilde{p_2}H = \\tilde{p_1},\n",
    "$$\n",
    "<br>\n",
    "where $\\tilde{p_1}$ and $\\tilde{p_2}$ are homogenous coordinates of $p_1$ and $p_2$.\n",
    "Note that it may be impossible to find the transformation $H$ that maps every point in image 2 exactly to the corresponding point in image 1. However, we can estimate the transformation matrix with least squares. Given $N$ matched keypoint pairs, let $X_1$ and $X_2$ be $N \\times 3$ matrices whose rows are homogenous coordinates of corresponding keypoints in image 1 and image 2 respectively. Then, we can estimate $H$ by solving the least squares problem,\n",
    "$$\n",
    "X_2 H = X_1\n",
    "$$\n",
    "<br>\n",
    "**This part has been implemented for you**. Run the following code to apply it to images. The images will be warped and the two images are merged to get a panorama. Your panorama may not look good at this point, but we will later use other techniques to get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from image_stitching import fit_affine_matrix\n",
    "from utils import get_output_space, warp_image\n",
    "\n",
    "# Extract matched keypoints\n",
    "p1 = keypoints1[matches[:,0]]\n",
    "p2 = keypoints2[matches[:,1]]\n",
    "\n",
    "# Find affine transformation matrix H that maps p2 to p1\n",
    "H = fit_affine_matrix(p1, p2)\n",
    "\n",
    "output_shape, offset = get_output_space(img1, [img2], [H])\n",
    "print(\"Output shape:\", output_shape)\n",
    "print(\"Offset:\", offset)\n",
    "\n",
    "# Warp images into output sapce\n",
    "img1_warped = warp_image(img1, np.eye(3), output_shape, offset)\n",
    "img1_mask = (img1_warped != -1) # Mask == 1 inside the image\n",
    "img1_warped[~img1_mask] = 0     # Return background values to 0\n",
    "\n",
    "img2_warped = warp_image(img2, H, output_shape, offset)\n",
    "img2_mask = (img2_warped != -1) # Mask == 1 inside the image\n",
    "img2_warped[~img2_mask] = 0     # Return background values to 0\n",
    "\n",
    "# Plot warped images\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1_warped)\n",
    "plt.title('Image 1 warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2_warped)\n",
    "plt.title('Image 2 warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged = img1_warped + img2_warped\n",
    "\n",
    "# Track the overlap by adding the masks together\n",
    "overlap = (img1_mask * 1.0 +  # Multiply by 1.0 for bool -> float conversion\n",
    "           img2_mask)\n",
    "\n",
    "# Normalize through division by `overlap` - but ensure the minimum is 1\n",
    "normalized = merged / np.maximum(overlap, 1)\n",
    "plt.imshow(normalized)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion (6 points)\n",
    "\n",
    "1. What are the disadvantage(s) of naive descriptor implemented in `simple_descriptor`, which takes the region around the keypoint as feature vector? Is it invariant to scale/rotation change? Give an example of a better approach.\n",
    "\n",
    "2. As showed above, the image stitching does not work well as we expected to produce a smooth panorama. In fact, it is caused by Image 2 that skews largely. Why does it happen? What are the possible solution(s)? \n",
    "\n",
    "Your answers:\n",
    "<br> (Delete all the < br> and this line before filling your answer)\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "END OF YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: RANSAC (RANdom SAmple Consensus) (22 points)\n",
    "\n",
    "The results showed above may not be so pleasant. RANSAC is a method to exclude `outliers`, by iteratively finding out a set of *good fits* and use the best fit to do keypoint matching.\n",
    "\n",
    "The steps of RANSAC are basically:\n",
    "1. Select random set of matches\n",
    "2. Compute affine transformation matrix\n",
    "3. Find inliers using the given threshold\n",
    "4. Repeat and keep the largest set of inliers\n",
    "5. Re-compute least-squares estimate on all of the inliers\n",
    "\n",
    "**Implement the `ransac` function in `image_stitching.py`**, then run the following code segments to get a (hopefully) better panorama. (16 points) You can see the difference from the result we get without RANSAC. **It is a normal if you find your matched keypoints a bit different from the provided solution. **\n",
    "\n",
    "*Reference: M. A. Fischler and R. C. Bolles (June 1981). \"[Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography](https://www.sri.com/sites/default/files/publications/ransac-publication.pdf)\". Comm. of the ACM 24: 381--395.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from image_stitching import ransac\n",
    "H, robust_matches = ransac(keypoints1, keypoints2, matches, threshold=1)\n",
    "\n",
    "# Visualize robust matches\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
    "plot_matches(ax, img1, img2, keypoints1, keypoints2, robust_matches)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('solution_ransac.jpg'))\n",
    "plt.axis('off')\n",
    "plt.title('RANSAC Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_shape, offset = get_output_space(img1, [img2], [H])\n",
    "\n",
    "# Warp images into output sapce\n",
    "img1_warped = warp_image(img1, np.eye(3), output_shape, offset)\n",
    "img1_mask = (img1_warped != -1) # Mask == 1 inside the image\n",
    "img1_warped[~img1_mask] = 0     # Return background values to 0\n",
    "\n",
    "img2_warped = warp_image(img2, H, output_shape, offset)\n",
    "img2_mask = (img2_warped != -1) # Mask == 1 inside the image\n",
    "img2_warped[~img2_mask] = 0     # Return background values to 0\n",
    "\n",
    "# Plot warped images\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1_warped)\n",
    "plt.title('Image 1 warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2_warped)\n",
    "plt.title('Image 2 warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged = img1_warped + img2_warped\n",
    "\n",
    "# Track the overlap by adding the masks together\n",
    "overlap = (img1_mask * 1.0 +  # Multiply by 1.0 for bool -> float conversion\n",
    "           img2_mask)\n",
    "\n",
    "# Normalize through division by `overlap` - but ensure the minimum is 1\n",
    "normalized = merged / np.maximum(overlap, 1)\n",
    "plt.imshow(normalized)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('solution_ransac_panorama.jpg'))\n",
    "plt.axis('off')\n",
    "plt.title('RANSAC Panorama Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion (6 points)\n",
    " \n",
    "As you may have found, although everytime RANSAC produces a different set of matched keypoints, it results in a similar stitched image. How does this happen? Refer to RANSAC's algorithm and discuss possible reason(s) here.\n",
    "\n",
    "Your answer:\n",
    "<br> (Delete all the < br> and this line before filling your answer)\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "END OF YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: SIFT (22 points)\n",
    "\n",
    "In the above code, you are using the `simple_descriptor`, and in this section, you are going to implement a simplified version of SIFT descriptor. (See Szeliski 4.1.2 or the original publications at http://www.cs.ubc.ca/~lowe/keypoints/)\n",
    "\n",
    "Here are the key properties you should implement for your (baseline) descriptor:\n",
    "1. A 4x4 grid of cells, each length of 16/4=4. It is simply the terminology used in the feature literature to describe the spatial bins where gradient distributions will be described.\n",
    "2. Each cell should have a histogram of the local distribution of gradients in 8 orientations. Appending these histograms together will give you 4x4 x 8 = 128 dimensions. *Hint: For an image sample, the gradient magnitude $m$ and orientation $\\theta$ are computed using pixel differences:*\n",
    "\n",
    "    $$m(x, y) = \\sqrt{(L(x+1,y)-L(x-1,y))^2 + (L(x,y+1)-L(x,y-1))^2}$$\n",
    "    $$\\theta(x,y) = atan2(L(x,y+1)-L(x,y-1), L(x+1,y)-L(x-1,y))$$\n",
    "<br>\n",
    "3. Feature should be normalized to unit length.\n",
    "\n",
    "You are **NOT** required (but feel free) to implement the following features:\n",
    "1. perform the interpolation in which each gradient measurement contributes to multiple orientation bins in multiple cells. As described in Szeliski, a single gradient measurement creates a weighted contribution to the 4 nearest cells and the 2 nearest orientation bins within each cell, for 8 total contributions. This type of interpolation probably will help, though.\n",
    "2. explicitly compute the gradient orientation at each pixel (although you are free to do so). You can instead filter with oriented filters (e.g. a filter that responds to edges with a specific orientation). All of your SIFT-like feature can be constructed entirely from filtering fairly quickly in this way.\n",
    "3. do the normalize -> threshold -> normalize again operation as detailed in Szeliski and the SIFT paper. It can help, though.\n",
    "\n",
    "Another simple trick which can help is to raise each element of the final feature vector to some power that is less than one.\n",
    "\n",
    "Implement **`sift_descriptor`** in `image_stitching.py`, and run through the following code to get a panorama image.\n",
    "\n",
    "-Hint: You can refer to https://en.wikipedia.org/wiki/Scale-invariant_feature_transform#Orientation_assignment for more details about how to compute *magnitude* and *orientation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from image_stitching import sift_descriptor\n",
    "\n",
    "img1 = imread('marinabay_a.jpg', as_grey=True)\n",
    "img2 = imread('marinabay_b.jpg', as_grey=True)\n",
    "\n",
    "# Detect keypoints in both images\n",
    "keypoints1 = corner_peaks(harris_corners(img1, window_size=3),\n",
    "                          threshold_rel=0.05,\n",
    "                          exclude_border=8)\n",
    "keypoints2 = corner_peaks(harris_corners(img2, window_size=3),\n",
    "                          threshold_rel=0.05,\n",
    "                          exclude_border=8)\n",
    "\n",
    "# Extract features from the corners\n",
    "desc1 = describe_keypoints(img1, keypoints1,\n",
    "                           desc_func=sift_descriptor,\n",
    "                           patch_size=16)\n",
    "desc2 = describe_keypoints(img2, keypoints2,\n",
    "                           desc_func=sift_descriptor,\n",
    "                           patch_size=16)\n",
    "\n",
    "# Match descriptors in image1 to those in image2\n",
    "matches = match_descriptors(desc1, desc2, 0.7)\n",
    "\n",
    "# Plot matches\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
    "ax.axis('off')\n",
    "plot_matches(ax, img1, img2, keypoints1, keypoints2, matches)\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('solution_sift.jpg'))\n",
    "plt.axis('off')\n",
    "plt.title('SIFT descriptor Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the results of matching with Least Square Method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p1 = keypoints1[matches[:,0]]\n",
    "p2 = keypoints2[matches[:,1]]\n",
    "H = fit_affine_matrix(p1, p2)\n",
    "\n",
    "output_shape, offset = get_output_space(img1, [img2], [H])\n",
    "\n",
    "# Warp images into output sapce\n",
    "img1_warped = warp_image(img1, np.eye(3), output_shape, offset)\n",
    "img1_mask = (img1_warped != -1) # Mask == 1 inside the image\n",
    "img1_warped[~img1_mask] = 0     # Return background values to 0\n",
    "\n",
    "img2_warped = warp_image(img2, H, output_shape, offset)\n",
    "img2_mask = (img2_warped != -1) # Mask == 1 inside the image\n",
    "img2_warped[~img2_mask] = 0     # Return background values to 0\n",
    "\n",
    "# Plot warped images\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1_warped)\n",
    "plt.title('Image 1 warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2_warped)\n",
    "plt.title('Image 2 warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged = img1_warped + img2_warped\n",
    "\n",
    "# Track the overlap by adding the masks together\n",
    "overlap = (img1_mask * 1.0 +  # Multiply by 1.0 for bool -> float conversion\n",
    "           img2_mask)\n",
    "\n",
    "# Normalize through division by `overlap` - but ensure the minimum is 1\n",
    "normalized = merged / np.maximum(overlap, 1)\n",
    "plt.imshow(normalized)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('solution_sift_panorama.jpg'))\n",
    "plt.axis('off')\n",
    "plt.title('SIFT Descriptor Panorama Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion (6 points)\n",
    "\n",
    "As the results showed above, SIFT seems to work good even when matching with least square method. What are the advantages of `sift_descriptor` over `simple_discriptor`? Discuss possible reason(s).\n",
    "\n",
    "Your answer:\n",
    "<br> (Delete all the < br> before filling your answer)\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "END OF YOUR ANSWER"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs4243",
   "language": "python",
   "name": "cs4243"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
